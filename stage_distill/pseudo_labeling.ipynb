{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, LlamaModel, AutoModelForSequenceClassification\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from accelerate import PartialState\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "from time import time\n",
    "import re\n",
    "import gc\n",
    "from threading import Thread\n",
    "import os\n",
    "os.chdir(\"/group-volume/binfeng/wsdm/stage_distill\")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/group-volume/binfeng/wsdm/ckpt/qwencd32b_ft/checkpoint-749\"\n",
    "SAVE_NAME = \"qwencd32b\"\n",
    "USE_DEVICES = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
    "\n",
    "MAX_LENGTH = 2000\n",
    "MAX_PROMPT_LENGTH = 400\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def split_col(col, idx):\n",
    "    return col[idx]\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, texts):\n",
    "    res = []\n",
    "    for text in texts:\n",
    "        input_ids = tokenizer(text)['input_ids']\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        res.append(input_ids)\n",
    "    return res\n",
    "\n",
    "\n",
    "def format_text(tokenizer, prompt, response_a, response_b, max_len=2000, max_prompt_len=400, reverse=False, bidirect=False):\n",
    "\n",
    "    enc_prompt, enc_response_a, enc_response_b = tokenizer.encode(\n",
    "        prompt), tokenizer.encode(response_a), tokenizer.encode(response_b)\n",
    "    max_len = max_len - 50  # leave space for special tokens\n",
    "    if len(enc_prompt) + len(enc_response_a) + len(enc_response_b) > max_len:\n",
    "        if len(enc_prompt) > max_prompt_len:\n",
    "            enc_prompt = enc_prompt[:max_prompt_len] + \\\n",
    "                tokenizer.encode(\" (truncated)\")\n",
    "        prompt_len, response_a_len, response_b_len = len(\n",
    "            enc_prompt), len(enc_response_a), len(enc_response_b)\n",
    "        # dynamic truncation to balance the length of responses\n",
    "        trunc_a, trunc_b = False, False\n",
    "        while prompt_len + response_a_len + response_b_len > max_len:\n",
    "            if response_a_len > response_b_len:\n",
    "                enc_response_a = enc_response_a[:-1]\n",
    "                response_a_len -= 1\n",
    "                trunc_a = True\n",
    "            else:\n",
    "                enc_response_b = enc_response_b[:-1]\n",
    "                response_b_len -= 1\n",
    "                trunc_b = True\n",
    "        prompt, response_a, response_b = tokenizer.decode(enc_prompt), tokenizer.decode(\n",
    "            enc_response_a), tokenizer.decode(enc_response_b)\n",
    "        if trunc_a:\n",
    "            response_a = response_a + \" (truncated)\"\n",
    "        if trunc_b:\n",
    "            response_b = response_b + \" (truncated)\"\n",
    "\n",
    "    prompt_format = \"## User Prompt\\n{prompt}\\n\\n## Response A\\n{response_a}\\n\\n## Response B\\n{response_b}.\\n\\n## Which response is better?\\n\"\n",
    "    if bidirect:\n",
    "        return [prompt_format.format(prompt=prompt, response_a=response_a, response_b=response_b),\n",
    "                prompt_format.format(prompt=prompt, response_a=response_b, response_b=response_a)]\n",
    "\n",
    "    if not reverse:\n",
    "        return prompt_format.format(prompt=prompt, response_a=response_a, response_b=response_b)\n",
    "    else:\n",
    "        return prompt_format.format(prompt=prompt, response_a=response_b, response_b=response_a)\n",
    "\n",
    "\n",
    "def format_label(winner, reverse=False, bidirect=False):\n",
    "    if bidirect:\n",
    "        return [int(0) if winner == \"model_a\" else int(1),\n",
    "                int(1) if winner == \"model_a\" else int(0)]\n",
    "    if not reverse:\n",
    "        return int(0) if winner == \"model_a\" else int(1)\n",
    "    else:\n",
    "        return int(1) if winner == \"model_a\" else int(0)\n",
    "\n",
    "\n",
    "def process_df(data):\n",
    "    data[\"tmp\"] = data.apply(lambda x: format_text(tokenizer, x.prompt, x.response_a, x.response_b,\n",
    "                                                   max_len=MAX_LENGTH, max_prompt_len=MAX_PROMPT_LENGTH,\n",
    "                                                   bidirect=True), axis=1)\n",
    "    data[\"text\"] = data.apply(lambda x: split_col(x.tmp, 0), axis=1)\n",
    "    data[\"text_reverse\"] = data.apply(lambda x: split_col(x.tmp, 1), axis=1)\n",
    "    data['text_len'] = data['text'].apply(lambda x: len(x.split(' ')))\n",
    "    data[\"input_ids\"] = tokenize(tokenizer, data[\"text\"])\n",
    "    data[\"input_ids_reverse\"] = tokenize(tokenizer, data[\"text_reverse\"])\n",
    "    data = data.sort_values(\"text_len\", ascending=False)\n",
    "    data = data.drop([\"tmp\", \"text_len\"], axis=1)\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.input_ids = df[\"input_ids\"].tolist()\n",
    "        self.input_ids_reverse = df[\"input_ids_reverse\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return both normal and reversed input_ids\n",
    "        if isinstance(idx, int):\n",
    "            return {\n",
    "                \"input_ids\": self.input_ids[idx],\n",
    "                \"input_ids_reverse\": self.input_ids_reverse[idx]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": [self.input_ids[i] for i in idx],\n",
    "                \"input_ids_reverse\": [self.input_ids_reverse[i] for i in idx]\n",
    "            }\n",
    "\n",
    "\n",
    "def load_model(device):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        num_labels=2,\n",
    "        # load_in_8bit=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    input_ids_reverse = [b[\"input_ids_reverse\"] for b in batch]\n",
    "\n",
    "    # Pad both normal and reversed sequences\n",
    "    normal_batch = pad_without_fast_tokenizer_warning(\n",
    "        tokenizer,\n",
    "        {\"input_ids\": input_ids},\n",
    "        padding=\"longest\",\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    reverse_batch = pad_without_fast_tokenizer_warning(\n",
    "        tokenizer,\n",
    "        {\"input_ids\": input_ids_reverse},\n",
    "        padding=\"longest\",\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"normal\": normal_batch,\n",
    "        \"reverse\": reverse_batch\n",
    "    }\n",
    "\n",
    "\n",
    "def inference_parallel(df, use_devices=['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']):\n",
    "    models = []\n",
    "    for device in use_devices:\n",
    "        print(f\"Loading model on {device}\")\n",
    "        models.append(load_model(device))\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['fold'] = [i % len(use_devices) for i in range(len(df))]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def run_inference_thread(sub_df, model, device):\n",
    "        dataset = CustomDataset(sub_df, tokenizer)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "        all_logits_normal = []\n",
    "        all_logits_reverse = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=f\"Device {device}\"):\n",
    "                # Process normal order\n",
    "                normal_batch = {k: v.to(device)\n",
    "                                for k, v in batch[\"normal\"].items()}\n",
    "                outputs_normal = model(**normal_batch)\n",
    "                logits_normal = outputs_normal.logits.float()\n",
    "                all_logits_normal.append(logits_normal.cpu().numpy())\n",
    "\n",
    "                # Process reverse order\n",
    "                reverse_batch = {k: v.to(device)\n",
    "                                 for k, v in batch[\"reverse\"].items()}\n",
    "                outputs_reverse = model(**reverse_batch)\n",
    "                logits_reverse = outputs_reverse.logits.float()\n",
    "                all_logits_reverse.append(logits_reverse.cpu().numpy())\n",
    "\n",
    "        all_logits_normal = np.concatenate(all_logits_normal, axis=0)\n",
    "        all_logits_reverse = np.concatenate(all_logits_reverse, axis=0)\n",
    "\n",
    "        # For reverse order, we need to swap the logits before averaging\n",
    "        swapped_logits_reverse = np.column_stack([\n",
    "            all_logits_reverse[:, 1],  # swap columns\n",
    "            all_logits_reverse[:, 0]\n",
    "        ])\n",
    "\n",
    "        # Average the logits\n",
    "        averaged_logits = (all_logits_normal + swapped_logits_reverse) / 2\n",
    "\n",
    "        sub_df['logits_model_a'] = averaged_logits[:, 0]\n",
    "        sub_df['logits_model_b'] = averaged_logits[:, 1]\n",
    "\n",
    "        results.append(sub_df)\n",
    "\n",
    "    threads = []\n",
    "    for idx, device in enumerate(use_devices):\n",
    "        sub_df = df[df['fold'] == idx].copy()\n",
    "        thread = Thread(target=run_inference_thread,\n",
    "                        args=(sub_df, models[idx], device))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    final_df = pd.concat(results, axis=0)\n",
    "    final_df = final_df.sort_index()\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data ...\n"
     ]
    }
   ],
   "source": [
    "print(\"processing data ...\")\n",
    "ppt = pd.read_csv(\"/user-volume/bx/ppt127k.csv\")\n",
    "kaggle = pd.read_csv(\"/user-volume/bx/kaggle48k.csv\")\n",
    "hfopen = pd.read_csv(\"/user-volume/bx/hfopen8k.csv\")\n",
    "lmsys = pd.read_parquet(\"/user-volume/bx/lmsys61k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencing lmsys set ... \n",
      "Loading model on cuda:0\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1764, in _get_module\n",
      "    self._modules = set(import_structure.keys())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 16, in <module>\n",
      "    from ...modeling_flash_attention_utils import FlashAttentionKwargs\n",
      "ImportError: cannot import name 'FlashAttentionKwargs' from 'transformers.modeling_flash_attention_utils' (/home/user/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1697/1619941758.py\", line 3, in <module>\n",
      "    res_lmsys = inference_parallel(lmsys, use_devices=USE_DEVICES)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1697/837311084.py\", line 184, in inference_parallel\n",
      "    models.append(load_model(device))\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1697/837311084.py\", line 141, in load_model\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n",
      "    model_class = _get_model_class(config, cls._model_mapping)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 388, in _get_model_class\n",
      "    supported_models = model_mapping[type(config)]\n",
      "                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 763, in __getitem__\n",
      "    return self._load_attr_from_module(model_type, model_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 777, in _load_attr_from_module\n",
      "    return getattribute_from_module(self._modules[module_name], attr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 693, in getattribute_from_module\n",
      "    if hasattr(module, attr):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1754, in __getattr__\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1766, in _get_module\n",
      "    for key, values in import_structure.items():\n",
      "RuntimeError: Failed to import transformers.models.qwen2.modeling_qwen2 because of the following error (look up to see its traceback):\n",
      "cannot import name 'FlashAttentionKwargs' from 'transformers.modeling_flash_attention_utils' (/home/user/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "print(\"inferencing lmsys set ... \")\n",
    "lmsys = process_df(lmsys)\n",
    "res_lmsys = inference_parallel(lmsys, use_devices=USE_DEVICES)\n",
    "res_lmsys.to_csv(\n",
    "    f\"/group-volume/binfeng/wsdm/stage_distill/plabels_v2/{SAVE_NAME}_plabel_lmsys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencing kaggle set ... \n",
      "Loading model on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:31<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:29<00:00,  2.09s/it]\n",
      "Device cuda:0:   0%|          | 0/6055 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device cuda:0:  29%|██▉       | 1745/6055 [2:23:50<6:01:22,  5.03s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferencing kaggle set ... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m kaggle \u001b[38;5;241m=\u001b[39m process_df(kaggle)\n\u001b[0;32m----> 3\u001b[0m res_kaggle \u001b[38;5;241m=\u001b[39m \u001b[43minference_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkaggle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_DEVICES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m res_kaggle\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/group-volume/binfeng/wsdm/stage_distill/plabels_v2/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAVE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_plabel_kaggle.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 246\u001b[0m, in \u001b[0;36minference_parallel\u001b[0;34m(df, use_devices)\u001b[0m\n\u001b[1;32m    243\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 246\u001b[0m     \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39msort_index()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device cuda:0:  29%|██▉       | 1746/6055 [2:23:55<5:55:11,  4.95s/it]\n",
      "Exception in thread Thread-7 (run_inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1131, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 518, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 646, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_60790/2458936389.py\", line 205, in run_inference_thread\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1327, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1293, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1144, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 68742) exited unexpectedly\n",
      "Device cuda:1:  28%|██▊       | 1723/6055 [2:23:58<6:01:59,  5.01s/it]\n",
      "Exception in thread Thread-8 (run_inference_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1131, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 518, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 646, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_60790/2458936389.py\", line 205, in run_inference_thread\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1327, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1293, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1144, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 68808) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "print(\"inferencing kaggle set ... \")\n",
    "kaggle = process_df(kaggle)\n",
    "res_kaggle = inference_parallel(kaggle, use_devices=USE_DEVICES)\n",
    "res_kaggle.to_csv(\n",
    "    f\"/group-volume/binfeng/wsdm/stage_distill/plabels_v2/{SAVE_NAME}_plabel_kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inferencing ppt set ... \")\n",
    "ppt = process_df(ppt)\n",
    "res_ppt = inference_parallel(ppt, use_devices=USE_DEVICES)\n",
    "res_ppt.to_csv(\n",
    "    f\"/group-volume/binfeng/wsdm/stage_distill/plabels_v2/{SAVE_NAME}_plabel_ppt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hfopen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hfopen \u001b[38;5;241m=\u001b[39m process_df(\u001b[43mhfopen\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferencing kaggle set ... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m res_kaggle \u001b[38;5;241m=\u001b[39m inference_parallel(kaggle, use_devices\u001b[38;5;241m=\u001b[39mUSE_DEVICES)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hfopen' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"inferencing hfopen set ... \")\n",
    "hfopen = process_df(hfopen)\n",
    "res_hfopen = inference_parallel(hfopen, use_devices=USE_DEVICES)\n",
    "res_hfopen.to_csv(\n",
    "    f\"/group-volume/binfeng/wsdm/stage_distill/plabels_v2/{SAVE_NAME}_plabel_hfopen.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
