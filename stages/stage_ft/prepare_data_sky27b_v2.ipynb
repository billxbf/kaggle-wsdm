{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import PartialState\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, LlamaModel, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "tqdm.pandas()\n",
    "\n",
    "# Change the working directory to the directory containing the script\n",
    "os.chdir(\"/group-volume/binfeng/wsdm/stage_ft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/group-volume/binfeng/wsdm/ckpt/sky27b_pptsmall/checkpoint-815\"\n",
    "MAX_LENGTH = 2000\n",
    "MAX_PROMPT_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "# tokenizer.save_pretrained(\"/group-volume/binfeng/wsdm/tokenizer/sky27b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9770 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "47952 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=100.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = pd.read_csv(\"/user-volume/bx/kaggle48k.csv\")\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "## REVERSED!\n",
    "data[\"text\"] = data.apply(lambda x: format_text(tokenizer, x.prompt, x.response_a, x.response_b, \n",
    "                                                max_len=MAX_LENGTH, max_prompt_len=MAX_PROMPT_LENGTH,\n",
    "                                                reverse=True), axis=1)\n",
    "data[\"label\"] = data.apply(lambda x: format_label(x.winner, \n",
    "                                                  reverse=True), axis=1)\n",
    "print(data[\"label\"].nunique())\n",
    "\n",
    "skf = StratifiedKFold(n_splits=100, shuffle=True, random_state=55)\n",
    "for train_index, val_index in skf.split(data, data[\"language\"]):\n",
    "    data_train, data_val = data.iloc[train_index], data.iloc[val_index]\n",
    "    print(len(data_train), len(data_val))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer_func(example):\n",
    "#     # Tokenize the input\n",
    "#     tokenized = tokenizer(\n",
    "#         example[\"text\"], \n",
    "#         padding='max_length', \n",
    "#         max_length=MAX_LENGTH,\n",
    "#         truncation=True,\n",
    "#         return_tensors='np'\n",
    "#     )\n",
    "    \n",
    "#     input_ids = tokenized['input_ids']\n",
    "#     attention_mask = tokenized['attention_mask']\n",
    "    \n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_mask\n",
    "#     }\n",
    "    \n",
    "def tokenizer_func(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=MAX_LENGTH, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 47952/47952 [00:34<00:00, 1389.95 examples/s]\n",
      "Map: 100%|██████████| 485/485 [00:00<00:00, 1324.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    kaggle48k_train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 47952\n",
       "    })\n",
       "    kaggle48k_val: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 485\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(data_train[[\"text\", \"label\"]])\n",
    "val_dataset = Dataset.from_pandas(data_val[[\"text\", \"label\"]])\n",
    "raw_dataset = DatasetDict({\n",
    "    'kaggle48k_train': train_dataset,\n",
    "    'kaggle48k_val': val_dataset\n",
    "})\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenizer_func, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['__index_level_0__'])\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>## User Prompt\n",
      "Where does elastic beanstalk store the environment variables for an environment? Give me the file path for Amazon Linux 2023\n",
      "\n",
      "## Response A\n",
      "In Amazon Linux 2023, AWS Elastic Beanstalk stores the environment variables for an environment in the following file:\n",
      "\n",
      "```\n",
      "/opt/elasticbeanstalk/deployment/env\n",
      "```\n",
      "\n",
      "This file contains all the environment variables that are set for your Elastic Beanstalk environment. You can access this file to inspect the environment variables that have been configured for your application.\n",
      "\n",
      "## Response B\n",
      "In Amazon Elastic Beanstalk (EB) on Amazon Linux 2 (which includes Amazon Linux 2023), environment variables can be stored and managed in several ways, but the primary locations where they are stored are:\n",
      "\n",
      "1. **`.ebextensions` Configuration Files**: These are YAML or JSON files that you place in your application's root directory. Environment variables defined here are persisted across deployments and are applied at the environment level. The specific file for environment variables is usually named `environment.yml` or `environment.json` under the `.ebextensions` directory. Here's an example of how you might define environment variables in a `environment.yml` file:\n",
      "\n",
      "    ```yaml\n",
      "    files:\n",
      "      \"/etc/profile.d/ebenv.sh\":\n",
      "        mode: \"000755\"\n",
      "        owner: root\n",
      "        group: root\n",
      "        content: |\n",
      "          export MY_ENV_VAR=value1\n",
      "          export ANOTHER_ENV_VAR=value2\n",
      "    ```\n",
      "\n",
      "2. **EB CLI Configuration**: You can also set environment variables using the EB CLI with the `eb config` command. These variables are stored locally on your machine in the EB CLI configuration file (`~/.ebextensions/ebconfig.ini`) and are associated with a specific environment. They are not persisted across machine restarts or if you use a different machine to manage your environment.\n",
      "\n",
      "3. **AWS Systems Manager Parameter Store**: For more secure and scalable management of environment variables, you can use AWS Systems Manager Parameter Store. You can reference parameters stored in the Parameter Store in your `.ebextensions` files or through environment variables set in the EB console or CLI. This method is useful for storing sensitive information like database credentials or API keys.\n",
      "\n",
      "Remember, the file path mentioned in the `.ebextensions` example (`/etc/profile.d/ebenv.sh`) is where the environment variables are set up on the instances running your application. This script is sourced by the shell when a new shell session is started, ensuring that the environment variables are available to your application..\n",
      "\n",
      "## Which response is better?\n",
      "<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
      "**label: 0\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(tokenizer.decode(tokenized_dataset[\"kaggle48k_val\"][i][\"input_ids\"], skip_special_tokens=False))\n",
    "print(\"**label:\", tokenized_dataset[\"kaggle48k_val\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 47952/47952 [00:00<00:00, 72728.23 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 485/485 [00:00<00:00, 50722.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"/group-volume/binfeng/wsdm/data/tokenized_sky27b_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.2498, -1.4825, -0.6032, -2.2185, -2.5468],\n",
       "         [-2.1985, -2.3299, -1.5729, -1.2161, -1.2451],\n",
       "         [-2.2972, -3.1037, -2.6995, -1.1984, -0.7222]],\n",
       "\n",
       "        [[-2.2661, -2.3434, -2.1984, -1.1783, -0.9636],\n",
       "         [-1.5325, -1.0846, -2.5868, -1.1091, -3.1976],\n",
       "         [-2.3467, -1.5876, -1.9824, -2.8244, -0.6875]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "log_probs = torch.log_softmax(torch.torch.randn(2,3,5), dim=-1)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]],\n",
       "\n",
       "        [[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
       "         [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dist = torch.ones_like(log_probs) / log_probs.size(-1)  # Shape: (2, 3, 5)\n",
    "target_log_dist = torch.log(target_dist)\n",
    "target_log_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3281, -0.0254, -0.2012,  0.1218,  0.1875],\n",
       "         [ 0.1178,  0.1441, -0.0073, -0.0787, -0.0729],\n",
       "         [ 0.1375,  0.2988,  0.2180, -0.0822, -0.1774]],\n",
       "\n",
       "        [[ 0.1313,  0.1468,  0.1178, -0.0862, -0.1292],\n",
       "         [-0.0154, -0.1050,  0.1955, -0.1001,  0.3176],\n",
       "         [ 0.1474, -0.0044,  0.0746,  0.2430, -0.1844]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dist * (target_log_dist - log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
