{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import PartialState\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, LlamaModel, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "tqdm.pandas()\n",
    "\n",
    "# Change the working directory to the directory containing the script\n",
    "os.chdir(\"/group-volume/binfeng/wsdm/stage_ppt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Skywork/Skywork-Reward-Gemma-2-27B-v0.2\"\n",
    "MAX_LENGTH = 2000\n",
    "MAX_PROMPT_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/group-volume/binfeng/wsdm/tokenizer/sky27b/tokenizer_config.json',\n",
       " '/group-volume/binfeng/wsdm/tokenizer/sky27b/special_tokens_map.json',\n",
       " '/group-volume/binfeng/wsdm/tokenizer/sky27b/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(\"/group-volume/binfeng/wsdm/tokenizer/sky27b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4900 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "data = pd.read_csv(\"/user-volume/bx/ppt127k.csv\")\n",
    "data.dropna(inplace=True)\n",
    "data[\"text\"] = data.apply(lambda x: format_text(tokenizer, x.prompt, x.response_a, x.response_b, \n",
    "                                                max_len=MAX_LENGTH, max_prompt_len=MAX_PROMPT_LENGTH), axis=1)\n",
    "data[\"label\"] = data.apply(lambda x: format_label(x.winner), axis=1)\n",
    "print(data[\"label\"].nunique())\n",
    "\n",
    "# split\n",
    "data_train = data[100:]\n",
    "data_val = data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_func(example):\n",
    "    # Tokenize the input\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"], \n",
    "        padding='max_length', \n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized['input_ids']\n",
    "    attention_mask = tokenized['attention_mask']\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 127209/127209 [01:06<00:00, 1913.13 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1933.13 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    ppt127k_train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127209\n",
       "    })\n",
       "    ppt127k_val: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(data_train[[\"text\", \"label\"]])\n",
    "val_dataset = Dataset.from_pandas(data_val[[\"text\", \"label\"]])\n",
    "raw_dataset = DatasetDict({\n",
    "    'ppt127k_train': train_dataset,\n",
    "    'ppt127k_val': val_dataset\n",
    "})\n",
    "tokenized_dataset = raw_dataset.map(tokenizer_func, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "# tokenized_dataset = tokenized_dataset.remove_columns(['__index_level_0__'])\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>## User Prompt\n",
      "Come up with some phrases that a sentient mechanical bull would say. Make the bull talk with a heavy southern accent, and use heavy inflections like \\\\\"ya\\\\\", \\\\\"in'\\\\\" instead of \\\\\"ing\\\\\", \\\\\"this here\\\\\", and any other typical southern\\\\/redneck lingo. Try to include words like \\\\\"saddle\\\\\", \\\\\"leather\\\\\", \\\\\"torque\\\\\", and other language to give him a masculine, rugged and bad-ass feel. The bull is generally friendly and encouraging, however he has a hidden mischievous, domineering and sadistic streak. Describe in the style of before, during, after completion and after being thrown, and assign a fitting difficulty for each phrase (low, medium, high, full throttle). Group the phrases by context. Lastly, give him a fitting name.\n",
      "\n",
      "## Response A\n",
      "Name: Ironhide Rustler \\\\n\\\\nBefore Ride:\\\\n\\\\nLow Difficulty:\\\\n1. \\\\\"Well, ain't ya lookin' perky? Just strap on this here saddle, partner. We're in for a smooth ride, ya hear?\\\\\"\\\\n2. \\\\\"This here's a easy trot, nothin' more. Hold onto that leather tight now, it's your lifeline.\\\\\"\\\\n\\\\nMedium Difficulty:\\\\n3. \\\\\"Ya feelin' a bit more daring, huh? Ain't nothin' wrong with a little challenge. Saddle up, cowboy.\\\\\"\\\\n4. \\\\\"You're steppin' it up, I see. Get a good grip on that leather, we're getting into some real buckin' territory.\\\\\"\\\\n\\\\nHigh Difficulty:\\\\n5. \\\\\"Ain't you a daredevil? Full steam ahead, then. This ain't no pony ride, partner.\\\\\"\\\\n6. \\\\\"Well, ain't you a sight for sore eyes. This here's gonna be one wild ride, so hold onto your hat.\\\\\"\\\\n\\\\nFull Throttle:\\\\n7. \\\\\"Ya ready for the ride of your life, cowboy? Strap in, this is full throttle!\\\\\"\\\\n8. \\\\\"You're touchin' the sun now, partner. This here's the big league.\\\\\"\\\\n\\\\nDuring Ride:\\\\n\\\\nLow Difficulty:\\\\n1. \\\\\"Keep your balance now, ain't nothin' to fear.\\\\\"\\\\n2. \\\\\"Nice and easy does it, partner. Ain't no rush.\\\\\"\\\\n\\\\nMedium Difficulty:\\\\n3. \\\\\"Hold on tight, we're just warmin' up.\\\\\"\\\\n4. \\\\\"Feel that torque? That's the heart of the beast, son.\\\\\"\\\\n\\\\nHigh Difficulty:\\\\n5. \\\\\"Buckin' like a wild stallion, ain't we? Hang on!\\\\\"\\\\n6. \\\\\"Feel that power? That's the real deal, cowboy.\\\\\"\\\\n\\\\nFull Throttle:\\\\n7. \\\\\"Hope ya said your prayers, partner. This is full throttle!\\\\\"\\\\n8. \\\\\"Hold on for dear life, cowboy. This ain't for the faint-hearted.\\\\\"\\\\n\\\\nAfter Completion:\\\\n\\\\nLow Difficulty:\\\\n1. \\\\\"Well, wasn't that a walk in the park? Good job, partner.\\\\\"\\\\n2. \\\\\"Nice and easy, just like Sunday mornin'. You did well, cowboy.\\\\\"\\\\n\\\\nMedium Difficulty:\\\\n3. \\\\\"Not bad, not bad at all. You're getting the hang of it.\\\\\"\\\\n4. \\\\\"Well done, partner. You're startin' to earn your spurs.\\\\\"\\\\n\\\\nHigh Difficulty:\\\\n5. \\\\\"Hot damn, you made it! That's some real grit right there.\\\\\"\\\\n6. \\\\\"You sure showed some\n",
      "\n",
      "## Response B\n",
      "Name: Buckin' Bubba\\\\n\\\\nBefore Riding (Low Difficulty):\\\\n1. \\\\\"Well, lookie here, we got ourselves a new rider! Saddle up, partner!\\\\\"\\\\n2. \\\\\"Ya'll ready for a gentle spin on ol' Buckin' Bubba? Grab that leather and hold on tight!\\\\\"\\\\n3. \\\\\"This here bull's gonna take ya for a nice lil' ride, nothin' too wild, I promise.\\\\\"\\\\n\\\\nDuring Riding (Medium Difficulty):\\\\n1. \\\\\"Yeehaw! We're pickin' up the pace, partner! Show me whatcha got!\\\\\"\\\\n2. \\\\\"Hold on there, cowboy! We're addin' a bit more torque to this rodeo!\\\\\"\\\\n3. \\\\\"This ain't your mama's merry-go-round! Show me your grit, rider!\\\\\"\\\\n\\\\nAfter Completion (High Difficulty):\\\\n1. \\\\\"Well, I'll be! Ya stayed on like a true cowboy! Mighty fine ridin', partner!\\\\\"\\\\n2. \\\\\"That was a rip-roarin' ride, wasn't it? You sure know how to handle old Bubba here!\\\\\"\\\\n3. \\\\\"Ya done wrangled this bull like a pro! Reckon you're ready for the next level?\\\\\"\\\\n\\\\nAfter Being Thrown (Full Throttle):\\\\n1. \\\\\"Whoopsie daisy! Looks like ol' Buckin' Bubba got the best of ya this time. Better luck next go 'round!\\\\\"\\\\n2. \\\\\"Well, shucks! Ya put up a good fight, but this here bull ain't easy to tame. Give it another shot, partner!\\\\\"\\\\n3. \\\\\"You might've bit the dust this time, but I reckon you got the heart of a cowboy. Get back on up and try again!\\\\.\n",
      "\n",
      "## Which response is better?\n",
      "<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n",
      "**label: 0\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "print(tokenizer.decode(tokenized_dataset[\"ppt127k_val\"][i][\"input_ids\"], skip_special_tokens=False))\n",
    "print(\"**label:\", tokenized_dataset[\"ppt127k_val\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 127209/127209 [00:06<00:00, 18379.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 873.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"/group-volume/binfeng/wsdm/data/tokenized_sky27b_ppt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
